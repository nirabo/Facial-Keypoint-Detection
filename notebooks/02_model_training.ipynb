{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Model Training\n",
    "\n",
    "This notebook demonstrates how to train a CNN model for facial keypoint detection.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Build a CNN architecture for keypoint regression\n",
    "- Train the model with data augmentation\n",
    "- Monitor training progress and evaluate performance\n",
    "- Save the trained model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Package imports\n",
    "from facial_keypoints.data.loader import load_data, get_data_statistics\n",
    "from facial_keypoints.visualization.plotting import plot_training_samples\n",
    "from facial_keypoints.config import settings\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "try:\n",
    "    X, y = load_data(test=False)\n",
    "    \n",
    "    # Split into train/validation sets (80/20)\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Validation samples: {len(X_val)}\")\n",
    "    print(f\"Input shape: {X_train.shape[1:]}\")\n",
    "    print(f\"Output shape: {y_train.shape[1:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load data: {e}\")\n",
    "    print(\"\\nTo use this notebook, download the dataset and place it in data/training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation\n",
    "\n",
    "Apply random transformations to increase training data diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmentation_layer():\n",
    "    \"\"\"Create a data augmentation layer for training.\"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.05),\n",
    "        layers.RandomBrightness(0.1),\n",
    "        layers.RandomContrast(0.1),\n",
    "    ], name=\"augmentation\")\n",
    "\n",
    "# Preview augmentation\n",
    "try:\n",
    "    augmentation = create_augmentation_layer()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    sample_img = X_train[0:1]\n",
    "    \n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i == 0:\n",
    "            ax.imshow(sample_img[0].squeeze(), cmap='gray')\n",
    "            ax.set_title('Original')\n",
    "        else:\n",
    "            augmented = augmentation(sample_img, training=True)\n",
    "            ax.imshow(augmented[0].squeeze(), cmap='gray')\n",
    "            ax.set_title(f'Augmented {i}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Data Augmentation Examples', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Data not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build CNN Model\n",
    "\n",
    "Architecture based on the original Udacity project with modern improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keypoint_model(input_shape=(96, 96, 1), n_keypoints=15):\n",
    "    \"\"\"Build a CNN model for facial keypoint detection.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input images (height, width, channels).\n",
    "        n_keypoints: Number of keypoints to predict.\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Conv Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Conv Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Conv Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Conv Block 4\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output: 2 coordinates per keypoint\n",
    "        layers.Dense(n_keypoints * 2, activation='tanh'),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display model\n",
    "model = build_keypoint_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 10\n",
    "\n",
    "# Create callbacks\n",
    "model_callbacks = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        'models/best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Early stopping patience: {PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=model_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"\\nTraining completed!\")\n",
    "except NameError:\n",
    "    print(\"Data not loaded - cannot train model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss (MSE)')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE\n",
    "    axes[1].plot(history.history['mae'], label='Training MAE')\n",
    "    axes[1].plot(history.history['val_mae'], label='Validation MAE')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].set_title('Training and Validation MAE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "try:\n",
    "    plot_training_history(history)\n",
    "except NameError:\n",
    "    print(\"Training history not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Evaluate model\n",
    "    val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"Validation Loss (MSE): {val_loss:.6f}\")\n",
    "    print(f\"Validation MAE: {val_mae:.6f}\")\n",
    "    \n",
    "    # Convert MAE to pixel error\n",
    "    pixel_mae = val_mae * 48  # Denormalize from [-1, 1] to [0, 96]\n",
    "    print(f\"\\nAverage pixel error: {pixel_mae:.2f} pixels\")\n",
    "except NameError:\n",
    "    print(\"Model not trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, X, y_true, n_samples=6):\n",
    "    \"\"\"Visualize model predictions vs ground truth.\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X[:n_samples], verbose=0)\n",
    "    \n",
    "    # Denormalize\n",
    "    y_true_pixels = y_true[:n_samples] * 48 + 48\n",
    "    y_pred_pixels = y_pred * 48 + 48\n",
    "    \n",
    "    # Plot\n",
    "    n_cols = 3\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4 * n_rows))\n",
    "    axes = axes.flatten() if n_samples > 1 else [axes]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(X[i].squeeze(), cmap='gray')\n",
    "        \n",
    "        # Ground truth (green)\n",
    "        ax.scatter(y_true_pixels[i, 0::2], y_true_pixels[i, 1::2],\n",
    "                   c='lime', s=30, marker='o', label='Ground Truth', edgecolors='black', linewidths=0.5)\n",
    "        \n",
    "        # Predictions (red)\n",
    "        ax.scatter(y_pred_pixels[i, 0::2], y_pred_pixels[i, 1::2],\n",
    "                   c='red', s=30, marker='x', label='Prediction', linewidths=1.5)\n",
    "        \n",
    "        ax.set_title(f'Sample {i}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for i in range(n_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Predictions (red) vs Ground Truth (green)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "try:\n",
    "    visualize_predictions(model, X_val, y_val, n_samples=6)\n",
    "except NameError:\n",
    "    print(\"Model or data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "try:\n",
    "    model_path = Path('models/keypoint_model.keras')\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    print(f\"Model size: {model_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "except NameError:\n",
    "    print(\"Model not available to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Preparation**: Loading and splitting data for training\n",
    "2. **Augmentation**: Using Keras preprocessing layers\n",
    "3. **Model Architecture**: Building a CNN for keypoint regression\n",
    "4. **Training**: Using callbacks for early stopping and checkpointing\n",
    "5. **Evaluation**: Visualizing predictions vs ground truth\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Proceed to `03_inference_pipeline.ipynb` to use the trained model\n",
    "- Experiment with different architectures or hyperparameters\n",
    "- Try transfer learning with pre-trained backbones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
