{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 - Model Training\n",
    "\n",
    "This notebook demonstrates how to train a CNN model for facial keypoint detection.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Build a CNN architecture for keypoint regression\n",
    "- Train the model with proper train/validation split\n",
    "- Monitor training progress and evaluate performance\n",
    "- Save the trained model for inference\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- **Input**: 96x96 grayscale images, normalized to [0, 1]\n",
    "- **Output**: 30 values (15 keypoints x 2 coordinates), normalized to [-1, 1]\n",
    "- **Training samples**: ~2140 (samples with all 15 keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Package imports\n",
    "from facial_keypoints.data.loader import load_data, get_data_statistics\n",
    "from facial_keypoints.visualization.plotting import plot_keypoints\n",
    "from facial_keypoints.config import settings\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Check TensorFlow and GPU\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs available: {len(gpus)}\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (already shuffled by load_data)\n",
    "X, y = load_data(test=False)\n",
    "\n",
    "print(f\"Loaded data shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Data types: X={X.dtype}, y={y.dtype}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = get_data_statistics(X, y)\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Samples: {stats['n_samples']}\")\n",
    "print(f\"  Keypoints: {stats['n_keypoints']}\")\n",
    "print(f\"  Image range: [{stats['x_min']:.3f}, {stats['x_max']:.3f}]\")\n",
    "print(f\"  Keypoint range: [{stats['y_min']:.3f}, {stats['y_max']:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation sets (80/20)\n",
    "VAL_SPLIT = 0.2\n",
    "split_idx = int(len(X) * (1 - VAL_SPLIT))\n",
    "\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)} ({100*(1-VAL_SPLIT):.0f}%)\")\n",
    "print(f\"Validation samples: {len(X_val)} ({100*VAL_SPLIT:.0f}%)\")\n",
    "print(f\"\\nInput shape: {X_train.shape[1:]}\")\n",
    "print(f\"Output shape: {y_train.shape[1:]} (15 keypoints x 2 coords)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Visualize Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few training samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    plot_keypoints(X_train[i], y_train[i], ax=ax, denormalize=True, \n",
    "                   marker_color='cyan', marker_size=30)\n",
    "    ax.set_title(f'Sample {i}')\n",
    "\n",
    "plt.suptitle('Training Samples with Ground Truth Keypoints', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Build CNN Model\n",
    "\n",
    "Architecture with 4 convolutional blocks followed by dense layers for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keypoint_model(input_shape=(96, 96, 1), n_keypoints=15):\n",
    "    \"\"\"Build a CNN model for facial keypoint detection.\n",
    "    \n",
    "    Architecture:\n",
    "    - 4 Conv blocks with increasing filters (32 -> 64 -> 128 -> 256)\n",
    "    - BatchNorm + MaxPool + Dropout in each block\n",
    "    - 2 Dense layers (512 -> 256) with dropout\n",
    "    - Output: n_keypoints * 2 with tanh activation (for [-1, 1] range)\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input images (height, width, channels).\n",
    "        n_keypoints: Number of keypoints to predict.\n",
    "        \n",
    "    Returns:\n",
    "        Compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Conv Block 1: 96x96 -> 48x48\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Conv Block 2: 48x48 -> 24x24\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Conv Block 3: 24x24 -> 12x12\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Conv Block 4: 12x12 -> 6x6\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Flatten: 6x6x256 = 9216 features\n",
    "        layers.Flatten(),\n",
    "        \n",
    "        # Dense Block 1\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Dense Block 2\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output: 2 coordinates per keypoint, tanh for [-1, 1] range\n",
    "        layers.Dense(n_keypoints * 2, activation='tanh'),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_keypoint_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.reduce_prod(w.shape).numpy() for w in model.trainable_weights])\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (estimated): {total_params * 4 / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 15  # Early stopping patience\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create callbacks\n",
    "training_callbacks = [\n",
    "    # Early stopping: stop if val_loss doesn't improve\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Reduce learning rate on plateau\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Save best model\n",
    "    callbacks.ModelCheckpoint(\n",
    "        str(models_dir / 'best_model.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Epochs (max): {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {len(X_train) // BATCH_SIZE}\")\n",
    "print(f\"Early stopping patience: {PATIENCE}\")\n",
    "print(f\"Model checkpoint: {models_dir / 'best_model.keras'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=training_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Total epochs trained: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(epochs, history.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss (MSE)')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find best epoch\n",
    "    best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    axes[0].axvline(best_epoch, color='green', linestyle='--', alpha=0.7, \n",
    "                    label=f'Best: epoch {best_epoch}')\n",
    "    axes[0].scatter([best_epoch], [best_val_loss], color='green', s=100, zorder=5)\n",
    "    \n",
    "    # MAE\n",
    "    axes[1].plot(epochs, history.history['mae'], 'b-', label='Training MAE', linewidth=2)\n",
    "    axes[1].plot(epochs, history.history['val_mae'], 'r-', label='Validation MAE', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE (normalized coords)')\n",
    "    axes[1].set_title('Training and Validation MAE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add secondary y-axis for pixel error\n",
    "    ax2 = axes[1].secondary_yaxis('right', functions=(lambda x: x * 48, lambda x: x / 48))\n",
    "    ax2.set_ylabel('MAE (pixels)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_epoch, best_val_loss\n",
    "\n",
    "best_epoch, best_val_loss = plot_training_history(history)\n",
    "print(f\"\\nBest model at epoch {best_epoch} with val_loss = {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "# Convert to pixel error\n",
    "pixel_mae = val_mae * 48  # Denormalize from [-1, 1] to pixel space\n",
    "pixel_rmse = np.sqrt(val_loss) * 48\n",
    "\n",
    "print(\"Validation Metrics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Loss (MSE): {val_loss:.6f}\")\n",
    "print(f\"MAE (normalized): {val_mae:.6f}\")\n",
    "print(f\"\\nPixel-space metrics (96x96 image):\")\n",
    "print(f\"  MAE: {pixel_mae:.2f} pixels\")\n",
    "print(f\"  RMSE: {pixel_rmse:.2f} pixels\")\n",
    "print(f\"\\nRelative error: {pixel_mae / 96 * 100:.1f}% of image size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, X, y_true, n_samples=8):\n",
    "    \"\"\"Visualize model predictions vs ground truth.\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X[:n_samples], verbose=0)\n",
    "    \n",
    "    # Denormalize to pixel coordinates\n",
    "    y_true_pixels = y_true[:n_samples] * 48 + 48\n",
    "    y_pred_pixels = y_pred * 48 + 48\n",
    "    \n",
    "    # Calculate per-sample error\n",
    "    errors = np.sqrt(np.mean((y_true_pixels - y_pred_pixels) ** 2, axis=1))\n",
    "    \n",
    "    # Plot\n",
    "    n_cols = 4\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 3.5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(X[i].squeeze(), cmap='gray')\n",
    "        \n",
    "        # Ground truth (green circles)\n",
    "        ax.scatter(y_true_pixels[i, 0::2], y_true_pixels[i, 1::2],\n",
    "                   c='lime', s=40, marker='o', label='Ground Truth', \n",
    "                   edgecolors='black', linewidths=0.5, zorder=10)\n",
    "        \n",
    "        # Predictions (red x markers)\n",
    "        ax.scatter(y_pred_pixels[i, 0::2], y_pred_pixels[i, 1::2],\n",
    "                   c='red', s=40, marker='x', label='Prediction', \n",
    "                   linewidths=2, zorder=11)\n",
    "        \n",
    "        ax.set_title(f'Sample {i} (RMSE: {errors[i]:.1f}px)')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for i in range(n_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Predictions (red X) vs Ground Truth (green O)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, X_val, y_val, n_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. Per-Keypoint Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error per keypoint\n",
    "y_pred_all = model.predict(X_val, verbose=0)\n",
    "\n",
    "# Denormalize\n",
    "y_true_pixels = y_val * 48 + 48\n",
    "y_pred_pixels = y_pred_all * 48 + 48\n",
    "\n",
    "# Reshape to (n_samples, 15, 2)\n",
    "y_true_kp = y_true_pixels.reshape(-1, 15, 2)\n",
    "y_pred_kp = y_pred_pixels.reshape(-1, 15, 2)\n",
    "\n",
    "# Calculate Euclidean distance per keypoint\n",
    "distances = np.sqrt(np.sum((y_true_kp - y_pred_kp) ** 2, axis=2))  # Shape: (n_samples, 15)\n",
    "\n",
    "# Mean error per keypoint\n",
    "mean_errors = distances.mean(axis=0)\n",
    "std_errors = distances.std(axis=0)\n",
    "\n",
    "# Keypoint names\n",
    "KEYPOINT_NAMES = [\n",
    "    'left_eye_center', 'right_eye_center', 'left_eye_inner', 'left_eye_outer',\n",
    "    'right_eye_inner', 'right_eye_outer', 'left_eyebrow_inner', 'left_eyebrow_outer',\n",
    "    'right_eyebrow_inner', 'right_eyebrow_outer', 'nose_tip',\n",
    "    'mouth_left', 'mouth_right', 'mouth_top', 'mouth_bottom'\n",
    "]\n",
    "\n",
    "print(\"Per-Keypoint Error (pixels)\")\n",
    "print(\"=\" * 50)\n",
    "for i, (name, mean_err, std_err) in enumerate(zip(KEYPOINT_NAMES, mean_errors, std_errors)):\n",
    "    bar = '#' * int(mean_err)\n",
    "    print(f\"{i:2d}. {name:22s}: {mean_err:5.2f} +/- {std_err:4.2f}  {bar}\")\n",
    "\n",
    "print(f\"\\nOverall mean: {mean_errors.mean():.2f} +/- {std_errors.mean():.2f} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize per-keypoint error\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(15)\n",
    "bars = ax.bar(x_pos, mean_errors, yerr=std_errors, capsize=4, color='steelblue', edgecolor='black')\n",
    "\n",
    "# Color bars by error magnitude\n",
    "for bar, err in zip(bars, mean_errors):\n",
    "    if err > 4:\n",
    "        bar.set_color('lightcoral')\n",
    "    elif err > 3:\n",
    "        bar.set_color('gold')\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(range(15))\n",
    "ax.set_xlabel('Keypoint Index')\n",
    "ax.set_ylabel('Mean Error (pixels)')\n",
    "ax.set_title('Per-Keypoint Prediction Error')\n",
    "ax.axhline(mean_errors.mean(), color='red', linestyle='--', label=f'Overall mean: {mean_errors.mean():.2f}px')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 10. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "final_model_path = models_dir / 'model.keras'\n",
    "model.save(final_model_path)\n",
    "\n",
    "print(f\"Model saved to: {final_model_path}\")\n",
    "print(f\"Model size: {final_model_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Also check best model\n",
    "best_model_path = models_dir / 'best_model.keras'\n",
    "if best_model_path.exists():\n",
    "    print(f\"\\nBest model (checkpointed): {best_model_path}\")\n",
    "    print(f\"Best model size: {best_model_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Preparation**: Loading and splitting data for training/validation\n",
    "2. **Model Architecture**: 4-block CNN with BatchNorm and Dropout\n",
    "3. **Training**: Using callbacks for early stopping, LR reduction, and checkpointing\n",
    "4. **Evaluation**: MSE/MAE metrics converted to pixel error\n",
    "5. **Visualization**: Comparing predictions vs ground truth\n",
    "6. **Error Analysis**: Per-keypoint error breakdown\n",
    "\n",
    "### Model Performance\n",
    "\n",
    "The model achieves reasonable accuracy for facial keypoint detection. Common error patterns:\n",
    "- Eyes and nose tip are typically predicted well\n",
    "- Mouth corners can be more challenging due to expression variation\n",
    "- Eyebrow endpoints may have higher variance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Proceed to `03_inference_pipeline.ipynb` to use the trained model on new images\n",
    "- Experiment with data augmentation for better generalization\n",
    "- Try different architectures (ResNet, EfficientNet) via transfer learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
